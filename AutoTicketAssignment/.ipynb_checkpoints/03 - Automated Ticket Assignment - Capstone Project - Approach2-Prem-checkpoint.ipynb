{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Automatic Ticket Assignment - Capstone Project - Approach2\n",
    "# \n",
    "# ## Problem Statement - \n",
    "# \n",
    "# In most of the IT organizations, the assignment of incidents to appropriate IT groups is still a manual process. Manual assignment of incidents is time consuming and requires human efforts. There may be mistakes due to human errors and resource consumption is carried out ineffectively because of the misaddressing. On the other hand, manual assignment increases the response and resolution times which result in user satisfaction deterioration / poor customer service. \n",
    "# \n",
    "# _<font color=blue>This capstone project intends to reduce the manual intervention of IT operations or Service desk teams by automating the ticket assignment process.The goal here is to create a text classification based ML model that can automatically  classify any new tickets by analysing ticket description to one of the relevant Assignment groups, which could be later integrated to any ITSM tool like Service Now. Based on the ticket description our model will output the probability of assigning it to one of the 74 Groups.</font>_\n",
    "# \n",
    "# The solution would be implemented using below approach:\n",
    "# \n",
    "# In the AS-IS process it's mentioned that around ~54% of the incidents are resolved by L1 / L2 teams and the rest will be resolved as L3. So the assumption is that GRP_0 and GRP_8 which contribute 54% of the tickets are related to L1/L2 teams and the rest of the tickets belongs to L3 teams\n",
    "# \n",
    "# So firstly, the ticket would be classified into L1/L2 or L3 classes and then it would be further classified into one of the given assignment groups. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Assignment group</th>\n",
       "      <th>cleaned_description</th>\n",
       "      <th>num_wds</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>uniq_wds</th>\n",
       "      <th>token_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-verified user details.(employee# &amp; manager na...</td>\n",
       "      <td>GRP_0</td>\n",
       "      <td>login issueverified user detailsemployee manag...</td>\n",
       "      <td>18</td>\n",
       "      <td>7.555556</td>\n",
       "      <td>15</td>\n",
       "      <td>login issueverified user detailsemployee manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\r\\n\\r\\nreceived from: hmjdrvpb.komuaywn@gmail...</td>\n",
       "      <td>GRP_0</td>\n",
       "      <td>outlook hmjdrvpbkomuaywn teammy meetingsskype ...</td>\n",
       "      <td>13</td>\n",
       "      <td>7.538462</td>\n",
       "      <td>12</td>\n",
       "      <td>outlook hmjdrvpbkomuaywn teammy meetingsskype ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\r\\n\\r\\nreceived from: eylqgodm.ybqkwiam@gmail...</td>\n",
       "      <td>GRP_0</td>\n",
       "      <td>cant log vpn eylqgodmybqkwiami cannot log vpn</td>\n",
       "      <td>7</td>\n",
       "      <td>5.571429</td>\n",
       "      <td>5</td>\n",
       "      <td>cant log vpn eylqgodmybqkwiami cannot log vpn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unable to access hr_tool page</td>\n",
       "      <td>GRP_0</td>\n",
       "      <td>unable access hrtool pageunable access hrtool ...</td>\n",
       "      <td>7</td>\n",
       "      <td>6.285714</td>\n",
       "      <td>5</td>\n",
       "      <td>unable access hrtool pageunable access hrtool ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>skype error</td>\n",
       "      <td>GRP_0</td>\n",
       "      <td>skype error skype error</td>\n",
       "      <td>4</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>skype error skype error</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description Assignment group  \\\n",
       "0  -verified user details.(employee# & manager na...            GRP_0   \n",
       "1  \\r\\n\\r\\nreceived from: hmjdrvpb.komuaywn@gmail...            GRP_0   \n",
       "2  \\r\\n\\r\\nreceived from: eylqgodm.ybqkwiam@gmail...            GRP_0   \n",
       "3                      unable to access hr_tool page            GRP_0   \n",
       "4                                       skype error             GRP_0   \n",
       "\n",
       "                                 cleaned_description  num_wds  avg_word  \\\n",
       "0  login issueverified user detailsemployee manag...       18  7.555556   \n",
       "1  outlook hmjdrvpbkomuaywn teammy meetingsskype ...       13  7.538462   \n",
       "2      cant log vpn eylqgodmybqkwiami cannot log vpn        7  5.571429   \n",
       "3  unable access hrtool pageunable access hrtool ...        7  6.285714   \n",
       "4                            skype error skype error        4  5.000000   \n",
       "\n",
       "   uniq_wds                                         token_desc  \n",
       "0        15  login issueverified user detailsemployee manag...  \n",
       "1        12  outlook hmjdrvpbkomuaywn teammy meetingsskype ...  \n",
       "2         5      cant log vpn eylqgodmybqkwiami cannot log vpn  \n",
       "3         5  unable access hrtool pageunable access hrtool ...  \n",
       "4         2                            skype error skype error  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_incidents_level = pd.read_csv('../dataset/cleaned_data.csv',encoding='utf-8')\n",
    "df_incidents_level.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents_level.rename(columns={'Assignment group':'Assignment_group'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Assignment_group</th>\n",
       "      <th>cleaned_description</th>\n",
       "      <th>num_wds</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>uniq_wds</th>\n",
       "      <th>token_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-verified user details.(employee# &amp; manager na...</td>\n",
       "      <td>GRP_0</td>\n",
       "      <td>login issueverified user detailsemployee manag...</td>\n",
       "      <td>18</td>\n",
       "      <td>7.555556</td>\n",
       "      <td>15</td>\n",
       "      <td>login issueverified user detailsemployee manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\r\\n\\r\\nreceived from: hmjdrvpb.komuaywn@gmail...</td>\n",
       "      <td>GRP_0</td>\n",
       "      <td>outlook hmjdrvpbkomuaywn teammy meetingsskype ...</td>\n",
       "      <td>13</td>\n",
       "      <td>7.538462</td>\n",
       "      <td>12</td>\n",
       "      <td>outlook hmjdrvpbkomuaywn teammy meetingsskype ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\r\\n\\r\\nreceived from: eylqgodm.ybqkwiam@gmail...</td>\n",
       "      <td>GRP_0</td>\n",
       "      <td>cant log vpn eylqgodmybqkwiami cannot log vpn</td>\n",
       "      <td>7</td>\n",
       "      <td>5.571429</td>\n",
       "      <td>5</td>\n",
       "      <td>cant log vpn eylqgodmybqkwiami cannot log vpn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unable to access hr_tool page</td>\n",
       "      <td>GRP_0</td>\n",
       "      <td>unable access hrtool pageunable access hrtool ...</td>\n",
       "      <td>7</td>\n",
       "      <td>6.285714</td>\n",
       "      <td>5</td>\n",
       "      <td>unable access hrtool pageunable access hrtool ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>skype error</td>\n",
       "      <td>GRP_0</td>\n",
       "      <td>skype error skype error</td>\n",
       "      <td>4</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>skype error skype error</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description Assignment_group  \\\n",
       "0  -verified user details.(employee# & manager na...            GRP_0   \n",
       "1  \\r\\n\\r\\nreceived from: hmjdrvpb.komuaywn@gmail...            GRP_0   \n",
       "2  \\r\\n\\r\\nreceived from: eylqgodm.ybqkwiam@gmail...            GRP_0   \n",
       "3                      unable to access hr_tool page            GRP_0   \n",
       "4                                       skype error             GRP_0   \n",
       "\n",
       "                                 cleaned_description  num_wds  avg_word  \\\n",
       "0  login issueverified user detailsemployee manag...       18  7.555556   \n",
       "1  outlook hmjdrvpbkomuaywn teammy meetingsskype ...       13  7.538462   \n",
       "2      cant log vpn eylqgodmybqkwiami cannot log vpn        7  5.571429   \n",
       "3  unable access hrtool pageunable access hrtool ...        7  6.285714   \n",
       "4                            skype error skype error        4  5.000000   \n",
       "\n",
       "   uniq_wds                                         token_desc  \n",
       "0        15  login issueverified user detailsemployee manag...  \n",
       "1        12  outlook hmjdrvpbkomuaywn teammy meetingsskype ...  \n",
       "2         5      cant log vpn eylqgodmybqkwiami cannot log vpn  \n",
       "3         5  unable access hrtool pageunable access hrtool ...  \n",
       "4         2                            skype error skype error  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_incidents_level.head()\n",
    "# Since the dataset is very imbalanced, we will be considering a subset of groups for predictions. In 74 groups, 46% of tickets belong to group 1 and 16 groups have more than 100 tickets and around 22 groups have more than 50 tickets, rest of the Assignment groups have very less ticket counts which might not add much value to the model prediction. If we conducted random sampling towards all the subcategories, then we would face a problem that we might miss all the tickets in some categories. Hence, we considered the groups that have more than 50 tickets in this appoach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents_level = df_incidents_level[df_incidents_level['Assignment_group'].map(df_incidents_level['Assignment_group'].value_counts()) > 50]\n",
    "\n",
    "df_incidents_level['Target'] = np.where(df_incidents_level['Assignment_group']=='GRP_0','L1/L2',np.where(df_incidents_level['Assignment_group'] =='GRP_8','L1/L2','L3'))\n",
    "x = df_incidents_level['token_desc']\n",
    "y = df_incidents_level['Target']\n",
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "# encoding train labels \n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=13,stratify=y)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8336489 , 1.24929064])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_array = np.ones(y_train.shape[0], dtype = 'float')\n",
    "for i, val in enumerate(y_train):\n",
    "    w_array[i] = class_weights[val]\n",
    "    \n",
    "    \n",
    "log_cols=[\"Classifier\", \"accuracy\",\"f1_score\"]\n",
    "log1 = pd.DataFrame(columns=log_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Classifier, accuracy, f1_score]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Modeling\n",
    "# \n",
    "# Pass the data to various models which learns to classify the tickets into one of the two groups -  L1/L2 or L3 class\n",
    "\n",
    "# #### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7800129785853342\n",
      "f1 score 0.7945710081662295\n",
      "logloss: 0.431 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.95      0.84       924\n",
      "           1       0.88      0.53      0.66       617\n",
      "\n",
      "    accuracy                           0.78      1541\n",
      "   macro avg       0.81      0.74      0.75      1541\n",
      "weighted avg       0.80      0.78      0.77      1541\n",
      "\n",
      "[[878  46]\n",
      " [293 324]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "predictions = nb.predict_proba(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) \n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "\n",
    "log_entry = pd.DataFrame([[\"MultinomialNB\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log1 = log1.append(log_entry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Linear SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8027255029201817\n",
      "f1 score 0.8010106025644771\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.78      0.83       924\n",
      "           1       0.72      0.83      0.77       617\n",
      "\n",
      "    accuracy                           0.80      1541\n",
      "   macro avg       0.80      0.81      0.80      1541\n",
      "weighted avg       0.81      0.80      0.80      1541\n",
      "\n",
      "[[724 200]\n",
      " [104 513]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(loss='hinge',random_state=42,class_weight='balanced'))),\n",
    "               ])\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "log_entry = pd.DataFrame([[\"LinearSVC\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log1 = log1.append(log_entry)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### SGD Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.755353666450357\n",
      "f1 score 0.7536760729524584\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.67      0.77       924\n",
      "           1       0.64      0.89      0.74       617\n",
      "\n",
      "    accuracy                           0.76      1541\n",
      "   macro avg       0.77      0.78      0.75      1541\n",
      "weighted avg       0.80      0.76      0.76      1541\n",
      "\n",
      "[[616 308]\n",
      " [ 69 548]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=100, tol=None,class_weight='balanced')),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"SGDClassifier\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log1 = log1.append(log_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7657365347177157\n",
      "f1 score 0.7636674268416505\n",
      "logloss: 1.877 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.75      0.79       924\n",
      "           1       0.68      0.79      0.73       617\n",
      "\n",
      "    accuracy                           0.77      1541\n",
      "   macro avg       0.76      0.77      0.76      1541\n",
      "weighted avg       0.78      0.77      0.77      1541\n",
      "\n",
      "[[694 230]\n",
      " [131 486]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_1 = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5,class_weight='balanced')),\n",
    "               ])\n",
    "logreg_1.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg_1.predict(X_test)\n",
    "predictions = logreg_1.predict_proba(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"LogisticRegression\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log1 = log1.append(log_entry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.755354</td>\n",
       "      <td>0.753676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.765737</td>\n",
       "      <td>0.763667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.780013</td>\n",
       "      <td>0.794571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.802726</td>\n",
       "      <td>0.801011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    accuracy  f1_score\n",
       "Classifier                            \n",
       "SGDClassifier       0.755354  0.753676\n",
       "LogisticRegression  0.765737  0.763667\n",
       "MultinomialNB       0.780013  0.794571\n",
       "LinearSVC           0.802726  0.801011"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log1.set_index([\"Classifier\"],inplace=True)\n",
    "log1.sort_values(by=['f1_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff2b94e8c88>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log1.sort_values(by=['f1_score']).plot(kind='barh',figsize=[7,6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC gives better performance compared to other models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l1_l2_classification.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Save the model\n",
    "# from sklearn.externals import joblib\n",
    "import sklearn.externals\n",
    "import joblib\n",
    "joblib.dump(svc, 'l1_l2_classification.pkl', compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L1/L2': 0, 'L3': 1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "le_name_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "le_name_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['L1/L2'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "model = joblib.load('l1_l2_classification.pkl')\n",
    "\n",
    "sentence = 'job failed in scheduler'\n",
    "encoder.inverse_transform(model.predict([sentence]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4621, 8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_incidents_l1_l2 = df_incidents_level[df_incidents_level['Target'] == 'L1/L2']\n",
    "df_incidents_l1_l2.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRP_0    3960\n",
       "GRP_8     661\n",
       "Name: Assignment_group, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_incidents_l1_l2.Assignment_group.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_incidents_l1_l2['token_desc']\n",
    "y = df_incidents_l1_l2['Assignment_group']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "# encoding train labels \n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=13,stratify=y)\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weights\n",
    "\n",
    "w_array = np.ones(y_train.shape[0], dtype = 'float')\n",
    "for i, val in enumerate(y_train):\n",
    "    w_array[i] = class_weights[val]\n",
    "    \n",
    "    \n",
    "log_cols=[\"Classifier\", \"accuracy\",\"f1_score\"]\n",
    "log2 = pd.DataFrame(columns=log_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9881081081081081\n",
      "f1 score 0.9883256723277415\n",
      "logloss: 0.044 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       793\n",
      "           1       1.00      0.92      0.96       132\n",
      "\n",
      "    accuracy                           0.99       925\n",
      "   macro avg       0.99      0.96      0.97       925\n",
      "weighted avg       0.99      0.99      0.99       925\n",
      "\n",
      "[[793   0]\n",
      " [ 11 121]]\n"
     ]
    }
   ],
   "source": [
    "# ### Naive Bayes\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = nb.predict(X_test)\n",
    "predictions = nb.predict_proba(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) \n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "\n",
    "log_entry = pd.DataFrame([[\"MultinomialNB\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log2 = log2.append(log_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.9913513513513513\n",
      "Test f1 score 0.9914647834785852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       793\n",
      "           1       1.00      0.94      0.97       132\n",
      "\n",
      "    accuracy                           0.99       925\n",
      "   macro avg       1.00      0.97      0.98       925\n",
      "weighted avg       0.99      0.99      0.99       925\n",
      "\n",
      "[[793   0]\n",
      " [  8 124]]\n"
     ]
    }
   ],
   "source": [
    "# ### Linear Support Vector Machine\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "svc = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(loss='hinge',random_state=42,class_weight='balanced'))),\n",
    "               ])\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "print('Test accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('Test f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"LinearSVC\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log2 = log2.append(log_entry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9902702702702703\n",
      "f1 score 0.9904145207350075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       793\n",
      "           1       1.00      0.93      0.96       132\n",
      "\n",
      "    accuracy                           0.99       925\n",
      "   macro avg       0.99      0.97      0.98       925\n",
      "weighted avg       0.99      0.99      0.99       925\n",
      "\n",
      "[[793   0]\n",
      " [  9 123]]\n"
     ]
    }
   ],
   "source": [
    "# ### SGD Classifier\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=100, tol=None,class_weight='balanced')),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "\n",
    "log_entry = pd.DataFrame([[\"SGDClassifier\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log2 = log2.append(log_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9913513513513513\n",
      "f1 score 0.9914647834785852\n",
      "logloss: 0.054 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       793\n",
      "           1       1.00      0.94      0.97       132\n",
      "\n",
      "    accuracy                           0.99       925\n",
      "   macro avg       1.00      0.97      0.98       925\n",
      "weighted avg       0.99      0.99      0.99       925\n",
      "\n",
      "[[793   0]\n",
      " [  8 124]]\n"
     ]
    }
   ],
   "source": [
    "# ### Logistic Regression\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_1 = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5,class_weight='balanced')),\n",
    "               ])\n",
    "logreg_1.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg_1.predict(X_test)\n",
    "predictions = logreg_1.predict_proba(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"LogisticRegression\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log2 = log2.append(log_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.988108</td>\n",
       "      <td>0.988326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.990270</td>\n",
       "      <td>0.990415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.991351</td>\n",
       "      <td>0.991465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.991351</td>\n",
       "      <td>0.991465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    accuracy  f1_score\n",
       "Classifier                            \n",
       "MultinomialNB       0.988108  0.988326\n",
       "SGDClassifier       0.990270  0.990415\n",
       "LinearSVC           0.991351  0.991465\n",
       "LogisticRegression  0.991351  0.991465"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log2.set_index([\"Classifier\"],inplace=True)\n",
    "log2.sort_values(by=['f1_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_l1_l2.pkl']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Save the model\n",
    "import joblib\n",
    "joblib.dump(logreg_1, 'model_l1_l2.pkl', compress=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GRP_0': 0, 'GRP_8': 1}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_name_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "le_name_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GRP_8'], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "model = joblib.load('model_l1_l2.pkl')\n",
    "\n",
    "sentence = 'job failed in scheduler'\n",
    "encoder.inverse_transform(model.predict([sentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Model to classify L3 tickets\n",
    "# \n",
    "# Lets's now train the models to classify the L3 tickets into one of the assignement groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents_l3 = df_incidents_level[df_incidents_level['Target'] == 'L3']\n",
    "df_incidents_l3 = df_incidents_l3[df_incidents_l3['Assignment_group'].map(df_incidents_l3['Assignment_group'].value_counts()) > 50]\n",
    "x = df_incidents_l3['token_desc']\n",
    "y = df_incidents_l3['Assignment_group']\n",
    "\n",
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "# encoding train labels \n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=13,stratify=y)\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weights\n",
    "\n",
    "w_array = np.ones(y_train.shape[0], dtype = 'float')\n",
    "for i, val in enumerate(y_train):\n",
    "    w_array[i] = class_weights[val]\n",
    "    \n",
    "log_cols=[\"Classifier\", \"accuracy\",\"f1_score\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.47649918962722854\n",
      "f1 score 0.5555073395313899\n",
      "logloss: 1.909 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.11      0.19        28\n",
      "           1       0.53      0.83      0.65        52\n",
      "           2       0.62      0.62      0.62        29\n",
      "           3       1.00      0.12      0.22        24\n",
      "           4       0.00      0.00      0.00        17\n",
      "           5       0.94      1.00      0.97        16\n",
      "           6       0.00      0.00      0.00        18\n",
      "           7       0.41      0.63      0.50        43\n",
      "           8       0.35      0.96      0.51        48\n",
      "           9       0.64      0.97      0.77        58\n",
      "          10       1.00      0.26      0.41        23\n",
      "          11       0.00      0.00      0.00        11\n",
      "          12       0.50      0.05      0.10        19\n",
      "          13       0.37      0.42      0.40        40\n",
      "          14       0.00      0.00      0.00        11\n",
      "          15       1.00      0.14      0.25        21\n",
      "          16       0.00      0.00      0.00        12\n",
      "          17       1.00      0.25      0.40        20\n",
      "          18       0.00      0.00      0.00        26\n",
      "          19       0.41      0.25      0.31        36\n",
      "          20       0.00      0.00      0.00        14\n",
      "          21       0.36      0.80      0.50        51\n",
      "\n",
      "    accuracy                           0.48       617\n",
      "   macro avg       0.45      0.34      0.31       617\n",
      "weighted avg       0.48      0.48      0.40       617\n",
      "\n",
      "[[ 3  0  2  0  0  0  0  0  2  3  0  0  0  0  0  0  0  0  0  0  0 18]\n",
      " [ 0 43  0  0  0  0  0  0  3  3  0  0  0  1  0  0  0  0  0  0  0  2]\n",
      " [ 1  0 18  0  0  0  0  1  5  1  0  0  0  0  0  0  0  0  0  2  0  1]\n",
      " [ 0  8  1  3  0  0  0  2  6  1  0  0  0  1  0  0  0  0  0  0  0  2]\n",
      " [ 0  3  0  0  0  0  0  3  9  0  0  0  0  2  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  2  3  0  0  0  0  0  5  0  0  0  0  1  0  0  0  0  0  7  0  0]\n",
      " [ 0  0  0  0  0  0  0 27  5  1  0  0  0 10  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  1 46  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  1  0 56  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  6  0  0  0  0  0  0  6  3  6  0  0  0  0  0  0  0  0  0  0  2]\n",
      " [ 0  2  0  0  0  0  0  1  8  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  2  0  0  0  0  2 11  0  0  0  1  0  0  0  0  0  0  3  0  0]\n",
      " [ 0  1  0  0  0  0  0 13  7  2  0  0  0 17  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  6  1  2  0  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  2  0 13  0  0  0  2  0  3  0  0  0  0  0  0]\n",
      " [ 0  5  1  0  0  0  0  0  3  3  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  5  0  0  0  0  0  2  4  0  0  0  0  4  0  0  0  5  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  2  0  0  0  0  2  0  0  0  0  0  0  0 22]\n",
      " [ 0  1  1  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  9  0 24]\n",
      " [ 0  0  0  0  0  1  0  3  5  0  0  0  0  3  0  0  0  0  0  1  0  1]\n",
      " [ 0  2  1  0  0  0  0  2  3  0  0  0  0  2  0  0  0  0  0  0  0 41]]\n"
     ]
    }
   ],
   "source": [
    "# ### Naive Bayes\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = nb.predict(X_test)\n",
    "predictions = nb.predict_proba(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) \n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"MultinomialNB\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.593192868719611\n",
      "Test f1 score 0.5999503491225455\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.82      0.39        28\n",
      "           1       0.68      0.77      0.72        52\n",
      "           2       0.55      0.59      0.57        29\n",
      "           3       0.55      0.50      0.52        24\n",
      "           4       0.81      0.76      0.79        17\n",
      "           5       1.00      1.00      1.00        16\n",
      "           6       0.63      0.67      0.65        18\n",
      "           7       0.47      0.58      0.52        43\n",
      "           8       0.62      0.83      0.71        48\n",
      "           9       0.94      0.86      0.90        58\n",
      "          10       0.82      0.61      0.70        23\n",
      "          11       0.62      0.45      0.53        11\n",
      "          12       0.68      0.68      0.68        19\n",
      "          13       0.40      0.47      0.44        40\n",
      "          14       0.36      0.36      0.36        11\n",
      "          15       0.61      0.52      0.56        21\n",
      "          16       0.20      0.08      0.12        12\n",
      "          17       0.85      0.55      0.67        20\n",
      "          18       0.67      0.08      0.14        26\n",
      "          19       0.50      0.36      0.42        36\n",
      "          20       0.92      0.86      0.89        14\n",
      "          21       0.93      0.25      0.40        51\n",
      "\n",
      "    accuracy                           0.59       617\n",
      "   macro avg       0.64      0.58      0.58       617\n",
      "weighted avg       0.66      0.59      0.59       617\n",
      "\n",
      "[[23  0  2  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  1  1  0  0]\n",
      " [ 2 40  0  0  0  0  0  1  1  2  0  0  0  4  0  1  1  0  0  0  0  0]\n",
      " [ 2  0 17  1  0  0  3  1  0  0  0  0  2  0  0  1  0  0  0  2  0  0]\n",
      " [ 1  3  2 12  0  0  0  1  1  0  0  0  0  1  0  1  0  1  0  0  0  1]\n",
      " [ 0  2  1  0 13  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  1  0  0 12  0  1  0  0  0  2  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0 25  3  0  0  0  0 12  0  0  1  0  0  0  1  0]\n",
      " [ 0  1  1  0  0  0  0  3 40  0  0  0  0  1  2  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  1  0 50  2  0  0  0  0  3  0  1  0  0  0  0]\n",
      " [ 0  1  2  1  0  0  1  0  4  0 14  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  1  0  0  1  0  2  0  0  5  0  0  0  0  1  0  0  0  0  0]\n",
      " [ 2  0  1  0  0  0  0  0  3  0  0  0 13  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  1  0  1 10  4  0  0  1  0 19  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  1  0  0  2  1  0  0  1  0  1  4  0  0  0  0  0  0  0]\n",
      " [ 0  2  0  0  0  0  0  4  0  1  1  0  0  1  0 11  1  0  0  0  0  0]\n",
      " [ 0  5  0  1  0  0  0  0  2  0  0  0  0  0  2  1  1  0  0  0  0  0]\n",
      " [ 0  2  1  2  0  0  0  2  0  0  0  0  0  2  0  0  0 11  0  0  0  0]\n",
      " [13  0  0  1  1  0  0  0  0  0  0  0  0  2  0  0  0  0  2  7  0  0]\n",
      " [19  0  1  0  0  0  1  0  0  0  0  0  2  0  0  0  0  0  0 13  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0  0  0  0  1  0  0  0  0  0  0 12  0]\n",
      " [27  1  2  0  0  0  0  2  2  0  0  0  0  1  0  0  0  0  0  3  0 13]]\n"
     ]
    }
   ],
   "source": [
    "# ### Linear Support Vector Machine\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "svc = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(loss='hinge',random_state=42,class_weight='balanced'))),\n",
    "               ])\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "print('Test accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('Test f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"LinearSVC\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5380875202593193\n",
      "f1 score 0.535327001211682\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.21      0.30        28\n",
      "           1       0.88      0.58      0.70        52\n",
      "           2       0.18      0.66      0.28        29\n",
      "           3       0.50      0.54      0.52        24\n",
      "           4       0.50      0.82      0.62        17\n",
      "           5       0.94      1.00      0.97        16\n",
      "           6       0.54      0.72      0.62        18\n",
      "           7       0.61      0.47      0.53        43\n",
      "           8       0.65      0.62      0.64        48\n",
      "           9       0.94      0.84      0.89        58\n",
      "          10       0.76      0.70      0.73        23\n",
      "          11       0.35      0.64      0.45        11\n",
      "          12       0.54      0.68      0.60        19\n",
      "          13       0.60      0.38      0.46        40\n",
      "          14       0.20      0.27      0.23        11\n",
      "          15       0.52      0.57      0.55        21\n",
      "          16       0.27      0.50      0.35        12\n",
      "          17       0.52      0.55      0.54        20\n",
      "          18       1.00      0.08      0.14        26\n",
      "          19       0.42      0.31      0.35        36\n",
      "          20       0.54      1.00      0.70        14\n",
      "          21       1.00      0.24      0.38        51\n",
      "\n",
      "    accuracy                           0.54       617\n",
      "   macro avg       0.59      0.56      0.53       617\n",
      "weighted avg       0.66      0.54      0.54       617\n",
      "\n",
      "[[ 6  0 17  0  0  0  0  0  0  0  1  2  0  0  0  0  0  1  0  1  0  0]\n",
      " [ 0 30  3  1  1  0  0  2  1  1  0  0  1  0  1  2  4  4  0  0  1  0]\n",
      " [ 1  0 19  0  1  0  2  0  1  0  0  1  1  0  0  0  0  0  0  3  0  0]\n",
      " [ 2  0  3 13  0  0  0  1  1  0  0  1  0  0  0  0  0  1  0  0  2  0]\n",
      " [ 0  0  1  1 14  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0 16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  1  0  0 13  0  0  0  0  1  2  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  2  1  0  2 20  3  0  0  0  1  6  3  1  2  0  0  0  2  0]\n",
      " [ 0  0  0  1  4  1  0  0 30  0  0  3  0  0  2  0  3  2  0  0  2  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 49  2  0  0  0  0  6  0  1  0  0  0  0]\n",
      " [ 0  0  2  1  2  0  1  0  1  0 16  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  1  0  0  1  0  0  0  0  7  0  0  0  0  1  0  0  0  0  0]\n",
      " [ 2  0  1  0  0  0  0  0  1  0  0  0 13  0  0  0  1  0  0  1  0  0]\n",
      " [ 0  0  0  0  3  0  1  6  3  1  1  3  1 15  3  0  1  0  0  0  2  0]\n",
      " [ 0  0  0  0  1  0  1  2  1  0  0  1  0  0  3  0  2  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  1  1  0  1  0  2 12  2  0  0  0  2  0]\n",
      " [ 0  1  0  1  0  0  0  0  2  0  0  0  0  0  1  1  6  0  0  0  0  0]\n",
      " [ 0  2  0  3  0  0  0  0  1  0  0  0  0  2  0  0  0 11  0  0  1  0]\n",
      " [ 0  0 14  1  1  0  0  0  0  0  0  0  0  0  0  1  0  0  2  7  0  0]\n",
      " [ 0  0 19  0  0  0  2  0  0  0  0  0  4  0  0  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 14  0]\n",
      " [ 0  1 28  0  0  0  1  2  1  0  0  1  0  2  0  0  0  0  0  3  0 12]]\n"
     ]
    }
   ],
   "source": [
    "# ### SGD Classifier\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=100, tol=None,class_weight='balanced')),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"SGDClassifier\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5769854132901134\n",
      "f1 score 0.5705393030481682\n",
      "logloss: 2.409 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.25      0.36        28\n",
      "           1       0.71      0.77      0.74        52\n",
      "           2       0.67      0.55      0.60        29\n",
      "           3       0.68      0.54      0.60        24\n",
      "           4       0.72      0.76      0.74        17\n",
      "           5       1.00      1.00      1.00        16\n",
      "           6       0.55      0.67      0.60        18\n",
      "           7       0.44      0.58      0.50        43\n",
      "           8       0.57      0.71      0.63        48\n",
      "           9       0.91      0.86      0.88        58\n",
      "          10       0.88      0.61      0.72        23\n",
      "          11       0.62      0.45      0.53        11\n",
      "          12       0.60      0.47      0.53        19\n",
      "          13       0.47      0.55      0.51        40\n",
      "          14       0.26      0.45      0.33        11\n",
      "          15       0.73      0.52      0.61        21\n",
      "          16       0.17      0.08      0.11        12\n",
      "          17       0.86      0.60      0.71        20\n",
      "          18       0.20      0.62      0.30        26\n",
      "          19       0.48      0.36      0.41        36\n",
      "          20       1.00      0.50      0.67        14\n",
      "          21       0.62      0.29      0.40        51\n",
      "\n",
      "    accuracy                           0.58       617\n",
      "   macro avg       0.63      0.56      0.57       617\n",
      "weighted avg       0.63      0.58      0.58       617\n",
      "\n",
      "[[ 7  0  1  0  0  0  1  1  0  0  0  1  0  0  0  0  0  0 16  1  0  0]\n",
      " [ 0 40  0  0  0  0  0  2  2  3  0  0  0  1  0  0  2  0  2  0  0  0]\n",
      " [ 1  0 16  0  0  0  3  0  2  0  0  0  1  0  0  1  0  0  1  2  0  2]\n",
      " [ 0  1  2 13  0  0  0  3  1  0  0  0  0  1  0  0  0  1  1  0  0  1]\n",
      " [ 0  2  1  0 13  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  2  0  0 12  0  1  0  0  0  2  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  1 25  3  0  0  0  0 10  2  0  1  0  0  0  0  0]\n",
      " [ 0  1  0  0  3  0  0  3 34  0  0  0  0  1  4  0  1  0  0  0  0  1]\n",
      " [ 0  2  0  0  0  0  0  2  0 50  1  0  0  0  0  2  0  1  0  0  0  0]\n",
      " [ 0  0  1  1  0  0  0  0  3  0 14  0  0  2  0  0  0  0  0  0  0  2]\n",
      " [ 0  1  0  0  0  0  1  0  3  0  0  5  0  0  0  0  0  0  0  0  0  1]\n",
      " [ 2  0  1  0  0  0  1  2  4  0  0  0  9  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  1  9  3  0  0  1  0 22  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  2  1  0  0  1  0  1  5  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  3  0  2  1  0  0  2  0 11  1  0  0  0  0  0]\n",
      " [ 0  5  0  0  0  0  0  1  1  0  0  0  0  0  2  1  1  0  0  1  0  0]\n",
      " [ 1  2  0  1  0  0  0  1  1  0  0  0  0  2  0  0  0 12  0  0  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0  0  0  0  0  1  0  0  0  0 16  7  0  1]\n",
      " [ 0  0  0  0  0  0  1  0  0  0  0  0  3  0  0  0  0  0 19 13  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0  0  0  0  2  3  0  0  0  0  0  7  1]\n",
      " [ 0  1  2  0  0  0  1  2  1  0  0  0  0  0  0  0  0  0 26  3  0 15]]\n"
     ]
    }
   ],
   "source": [
    "# ### Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_1 = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5,class_weight='balanced')),\n",
    "               ])\n",
    "logreg_1.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg_1.predict(X_test)\n",
    "predictions = logreg_1.predict_proba(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"LogisticRegression\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Classifier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.538088</td>\n",
       "      <td>0.535327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.476499</td>\n",
       "      <td>0.555507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.576985</td>\n",
       "      <td>0.570539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.576985</td>\n",
       "      <td>0.570539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.593193</td>\n",
       "      <td>0.599950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    accuracy  f1_score\n",
       "Classifier                            \n",
       "SGDClassifier       0.538088  0.535327\n",
       "MultinomialNB       0.476499  0.555507\n",
       "LogisticRegression  0.576985  0.570539\n",
       "LogisticRegression  0.576985  0.570539\n",
       "LinearSVC           0.593193  0.599950"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.set_index([\"Classifier\"],inplace=True)\n",
    "log.sort_values(by=['f1_score'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff2891144a8>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log.sort_values(by=['f1_score']).plot(kind='barh',figsize=[7,6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Hyperparameter Tune GridSearchCV \n",
    "# \n",
    "# Let's tune each of the models using GridsearchCV to identify the best parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:   23.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.5864182764039391\n",
      "Best Params:  {'clf__estimator__C': 1, 'clf__estimator__loss': 'squared_hinge'}\n"
     ]
    }
   ],
   "source": [
    "# ### Linear SVC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\"clf__estimator__C\": [0.1, 1, 10, 100, 1000],  \n",
    "              'clf__estimator__loss': ['hinge','squared_hinge'],}  \n",
    "  \n",
    "clf_svc = GridSearchCV(svc, param_grid=params, refit = True, verbose = 1,scoring='f1_weighted') \n",
    "# fitting the model for grid search \n",
    "clf_svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Score: \", clf_svc.best_score_)\n",
    "print(\"Best Params: \", clf_svc.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.593192868719611\n",
      "f1 score 0.5913354430373698\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.29      0.38        28\n",
      "           1       0.69      0.79      0.74        52\n",
      "           2       0.57      0.59      0.58        29\n",
      "           3       0.57      0.54      0.55        24\n",
      "           4       0.81      0.76      0.79        17\n",
      "           5       1.00      1.00      1.00        16\n",
      "           6       0.55      0.67      0.60        18\n",
      "           7       0.46      0.58      0.52        43\n",
      "           8       0.62      0.83      0.71        48\n",
      "           9       0.94      0.84      0.89        58\n",
      "          10       0.81      0.57      0.67        23\n",
      "          11       0.62      0.45      0.53        11\n",
      "          12       0.68      0.68      0.68        19\n",
      "          13       0.49      0.50      0.49        40\n",
      "          14       0.33      0.36      0.35        11\n",
      "          15       0.63      0.57      0.60        21\n",
      "          16       0.20      0.08      0.12        12\n",
      "          17       0.85      0.55      0.67        20\n",
      "          18       0.20      0.62      0.30        26\n",
      "          19       0.48      0.33      0.39        36\n",
      "          20       0.92      0.86      0.89        14\n",
      "          21       0.93      0.25      0.40        51\n",
      "\n",
      "    accuracy                           0.59       617\n",
      "   macro avg       0.63      0.58      0.58       617\n",
      "weighted avg       0.65      0.59      0.60       617\n",
      "\n",
      "[[ 8  0  1  0  0  0  1  0  0  0  0  1  0  0  0  0  0  0 16  1  0  0]\n",
      " [ 0 41  0  0  0  0  0  1  1  2  0  0  0  3  0  1  1  0  2  0  0  0]\n",
      " [ 2  0 17  1  0  0  3  1  0  0  0  0  1  0  0  1  0  0  1  2  0  0]\n",
      " [ 0  2  2 13  0  0  0  2  1  0  0  0  0  1  0  0  0  1  1  0  0  1]\n",
      " [ 0  2  1  0 13  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  1  1  0  0 12  0  1  0  0  0  2  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  1  0  0  1 25  3  0  0  0  1 10  0  0  1  0  0  0  1  0]\n",
      " [ 0  1  1  0  0  0  0  3 40  0  0  0  0  1  2  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  2  0 49  2  0  0  0  0  3  0  1  0  0  0  0]\n",
      " [ 1  1  2  1  0  0  1  0  4  0 13  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  1  0  0  1  0  2  0  0  5  0  0  0  0  1  0  0  0  0  0]\n",
      " [ 2  0  1  0  0  0  0  0  3  0  0  0 13  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  1 10  4  0  0  1  0 20  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  1  0  0  3  1  0  0  1  0  0  4  0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0  0  3  0  1  1  0  0  1  1 12  1  0  0  0  0  0]\n",
      " [ 0  5  0  1  0  0  0  0  2  0  0  0  0  0  2  1  1  0  0  0  0  0]\n",
      " [ 0  3  1  2  0  0  0  1  0  0  0  0  0  2  0  0  0 11  0  0  0  0]\n",
      " [ 0  0  0  1  1  0  0  0  0  0  0  0  0  0  0  1  0  0 16  7  0  0]\n",
      " [ 0  0  1  0  0  0  2  0  0  0  0  0  2  0  0  0  0  0 19 12  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0  0  0  0  1  0  0  0  0  0  0 12  0]\n",
      " [ 1  1  2  0  0  0  0  2  2  0  0  0  0  1  0  0  0  0 26  3  0 13]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_svc.best_estimator_.predict(X_test)\n",
    "#predictions = clf_svc.best_estimator_.predict_proba(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "#print (\"logloss: %0.3f \" % multiclass_logloss(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"LinearSVC_best_estimator\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols,index=['LinearSVC_best_estimator'])\n",
    "log = log.append(log_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "# ### SGD Classifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"clf__loss\" : [\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\"],\n",
    "    \"clf__alpha\" : [0.0001, 0.001, 0.01, 0.1],\n",
    "    \"clf__penalty\" : [\"l2\", \"l1\", \"none\"],\n",
    "}\n",
    "\n",
    "clf_sgd = GridSearchCV(sgd, param_grid=params,refit = True, verbose = 1,scoring='f1_weighted')\n",
    "clf_sgd.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Score: \", clf_sgd.best_score_)\n",
    "print(\"Best Params: \", clf_sgd.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_sgd.best_estimator_.predict(X_test)\n",
    "#predictions = clf_svc.best_estimator_.predict_proba(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "#print (\"logloss: %0.3f \" % multiclass_logloss(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"SGD_best_estimator\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols,index=['SGD_best_estimator'])\n",
    "log = log.append(log_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Logistic Regression\n",
    "\n",
    "\n",
    "\n",
    "params = {\n",
    "  'clf__penalty': ['l2'],\n",
    "  'clf__C': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0,1e2,1e4,1e5],\n",
    "  'clf__max_iter': [100,4000,5000],\n",
    "}\n",
    "\n",
    "clf_lr = GridSearchCV(logreg_1, param_grid=params,refit = True,verbose = 1,scoring='f1_weighted')\n",
    "clf_lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Score: \", clf_lr.best_score_)\n",
    "print(\"Best Params: \", clf_lr.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_lr.best_estimator_.predict(X_test)\n",
    "#predictions = clf_svc.best_estimator_.predict_proba(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "#print (\"logloss: %0.3f \" % multiclass_logloss(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"LogisticRegression_best_estimator\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols,index=['LogisticRegression_best_estimator'])\n",
    "log = log.append(log_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.sort_values(by=['f1_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.sort_values(by=['f1_score']).plot(kind='barh',figsize=[7,6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "class FocalLoss(keras.losses.Loss):\n",
    "    def __init__(self, gamma=2., alpha=4.,\n",
    "                 reduction=keras.losses.Reduction.AUTO, name='focal_loss'):\n",
    "        \"\"\"Focal loss for multi-classification\n",
    "        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
    "        Notice: y_pred is probability after softmax\n",
    "        gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper\n",
    "        d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)\n",
    "        Focal Loss for Dense Object Detection\n",
    "        https://arxiv.org/abs/1708.02002\n",
    "\n",
    "        Keyword Arguments:\n",
    "            gamma {float} -- (default: {2.0})\n",
    "            alpha {float} -- (default: {4.0})\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__(reduction=reduction,\n",
    "                                        name=name)\n",
    "        self.gamma = float(gamma)\n",
    "        self.alpha = float(alpha)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
    "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9\n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "\n",
    "        model_out = tf.add(y_pred, epsilon)\n",
    "        ce = tf.multiply(y_true, -tf.math.log(model_out))\n",
    "        weight = tf.multiply(y_true, tf.pow(\n",
    "            tf.subtract(1., model_out), self.gamma))\n",
    "        fl = tf.multiply(self.alpha, tf.multiply(weight, ce))\n",
    "        reduced_fl = tf.reduce_max(fl, axis=1)\n",
    "        return reduced_fl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "x = df_incidents_l3['token_desc']\n",
    "y = df_incidents_l3['Assignment_group']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.3, random_state=13,stratify=y)\n",
    "\n",
    "num_labels = 22\n",
    "vocab_size = 50000\n",
    "batch_size = 64\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    " \n",
    "x_train = tokenizer.texts_to_matrix(X_train, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(X_test, mode='tfidf')\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(30))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss=FocalLoss(alpha=1),\n",
    "              optimizer='nadam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=30,\n",
    "                    verbose=1,validation_split=0.2,callbacks=[es_callback])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    " \n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(clf_lr.best_estimator_, 'model_l3.pkl', compress=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Finals Conclusions fo Approach2\n",
    "# \n",
    "# - We first analysed the dataset provided to us, undestood the structure of the data provided - number of columns, field , datatypes etc.\n",
    "# - We did Exploratory Data Analysis to derive further insights from this data set and we found that\n",
    "#     - Data is very much imbalanced, there are around ~45% of the Groups with less than 20 tickets.\n",
    "#     - Few of the tickets are in foreign language like German\n",
    "#     - The data has lot of noise in it, for eg- few tickets related to account setup are spread across multiple assignment groups.\n",
    "#     \n",
    "# - We performed the data cleaning and preprocessing\n",
    "#     - Translation: A small number of tickets were written in German. Hence, we used the Google translate python api  to convert German to English to generate the input data for the next steps. However, the google translator rest api can only process a limited number of texts on a daily basis, so we translated the text in batches and saved the file for further processing.\n",
    "#     - Make text all lowercase so that the algorithm does not treat the same words in different cases as different\n",
    "#     - Removing Noise i.e everything that isn’t in a standard number or letter i.e Punctuation, Numerical values\n",
    "#     - Removing extract spaces\n",
    "#     - Removed punctuations\n",
    "#     - Removed words containing numbers\n",
    "#     - Stopword Removal: Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words\n",
    "#     - Lemmatization\n",
    "#     - Tokenization: Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e words that we actually want. Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings.\n",
    "#     \n",
    "# \n",
    "# - We then ran a basic benchmarck model using the cleaned and preprocessed dataset\n",
    "#     - Since the dataset is very imbalanced, We considered a subset of groups for predictions.  In 74 groups, 46% of tickets belong to group 1 and 16 groups just have more than 100 tickets, rest of the Assignment groups have very less ticket counts which might not add much value to the model prediction. If we conducted random sampling towards all the subcategories, then we would face a problem that we might miss all the tickets in some categories. Hence, we considered the groups that have more than 100 tickets. \n",
    "#     - We trained the data using below models:\n",
    "#         - Multinomial NB\n",
    "#         - Linear Support vector Machine\n",
    "#         - Logistic regression\n",
    "#         - Xgboost\n",
    "#         \n",
    "# -  LinearSVC gives better performance with \n",
    "#     accuracy 0.833642\n",
    "#     f1 score 0.818053\n",
    "# \n",
    "# <b> Although, it seems like the call is biased towards GRP_0 which has a majority of samples. </b>\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# - Even after downsampling the data we see that the predictions are biased towards GRP_0 which has a majority of samples.\n",
    "# - Imbalance causes two problems:\n",
    "#     - Training is inefficient as most samples are easy examples that contribute no useful learning signal;\n",
    "#     - The easy examples can overwhelm training and lead to degenerate models.\n",
    "#     A common solution is to perform some form of hard negative mining that samples hard examples during training or more complex sampling/re weighing schemes.In order to handle the imbalance problem  we used class_weight=balanced hyperparameter while training the model, which tells the model to \"pay more attention\" to samples from an under-represented class.  \n",
    "# - Although, the accuracy and f1_score went down. This ensured that the classes were being correctly classified with lesser number of missclassification and good precision/recall scores for all the classes\n",
    "# \n",
    "# - Next, we used Approach 2 where the ticket would be classified into L1/L2 or L3 classes and then it would be further classified into one of the given assignment groups. \n",
    "# \n",
    "# - We first created a model to classify the given tickets as l1/l2 or l3 ticket, we found that Linear SVC was giving a better score.\n",
    "# - Next, another model was trained considering only l1/l2 level of incidents consisting of GRP_0 and GRP_8.\n",
    "# - Finally, a third model was trained considering l3 level of tickets.\n",
    "# \n",
    "# - We also used hyperparameter tuning, to identify the best classifier with best parameters. We found that LinearSVC was performing the best among all the other classifiers.\n",
    "# - We also tried keras implementation with focal loss as a loss function to handle the class imbalance problem, which helps in giving more weightage to groups will less samples, but the results were not satifactory.\n",
    "#   \n",
    "#  - Finally, Logistic Regression gave better performance with hyperparameter tuning and this model would be used for classifying the L3 tickets into one of the groups.\n",
    "#     - accuracy 0.706260\n",
    "#     - f1 score 0.705392\n",
    "# \n",
    "# \n",
    "# The performance can be further improved by collecting more data for tickets and by running deep learning models like RNN and LSTM's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
