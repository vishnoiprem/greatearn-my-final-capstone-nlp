{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Automatic Ticket Assignment - Capstone Project - MEthod 1\n",
    "# \n",
    "# ## Problem Statement - \n",
    "# \n",
    "# In most of the IT organizations, the assignment of incidents to appropriate IT groups is still a manual process. Manual assignment of incidents is time consuming and requires human efforts. There may be mistakes due to human errors and resource consumption is carried out ineffectively because of the misaddressing. On the other hand, manual assignment increases the response and resolution times which result in user satisfaction deterioration / poor customer service. \n",
    "# \n",
    "# _<font color=blue>This capstone project intends to reduce the manual intervention of IT operations or Service desk teams by automating the ticket assignment process.The goal here is to create a text classification based ML model that can automatically  classify any new tickets by analysing ticket description to one of the relevant Assignment groups, which could be later integrated to any ITSM tool like Service Now. Based on the ticket description our model will output the probability of assigning it to one of the 74 Groups.</font>_\n",
    "# \n",
    "# The solution would be implemented using below approach:\n",
    "# - Approach 1 - Using a traditional machine learning algorithm we would be classifying the tickets into one of the groups having more than 100 tickets.\n",
    "# \n",
    "\n",
    "# ### Import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tranlated_inc = pd.read_csv('../dataset/cleaned_data.csv',encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tranlated_inc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tranlated_inc.rename(columns={'Assignment group':'Assignment_group'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tranlated_inc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inc_sample = df_tranlated_inc[df_tranlated_inc['Assignment_group'].map(df_tranlated_inc['Assignment_group'].value_counts()) > 100]\n",
    "x = df_inc_sample['token_desc']\n",
    "y = df_inc_sample['Assignment_group']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "# encoding train labels \n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=13,stratify=y)\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_array = np.ones(y_train.shape[0], dtype = 'float')\n",
    "for i, val in enumerate(y_train):\n",
    "    w_array[i] = class_weights[val]\n",
    "    \n",
    "log_cols=[\"Classifier\", \"accuracy\",\"f1_score\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Naive Bayes classifier for multinomial models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "predictions = nb.predict_proba(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test)) \n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_entry = pd.DataFrame([[\"MultinomialNB\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(loss='hinge',random_state=42,class_weight='balanced'))),\n",
    "               ])\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc.predict(X_test)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "#print (focal_loss(alpha=.25, gamma=2))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"LinearSVC\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=100, tol=None,class_weight='balanced')),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sgd.predict(X_test)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "log_entry = pd.DataFrame([[\"SGDClassifier\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_1 = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5,class_weight='balanced')),\n",
    "               ])\n",
    "logreg_1.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "y_pred = logreg_1.predict(X_test)\n",
    "predictions = logreg_1.predict_proba(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "log_entry = pd.DataFrame([[\"LogisticRegression\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Xgboost\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "xgboost = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)),\n",
    "               ])\n",
    "xgboost.fit(X_train, y_train,clf__sample_weight=w_array)\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "y_pred = xgboost.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"Xgboost\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Hyperparameter Tune GridSearchCV\n",
    "\n",
    "# ### LinearSVC\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\"clf__estimator__C\": [0.1, 1, 10, 100, 1000],  \n",
    "              'clf__estimator__loss': ['hinge','squared_hinge'],}  \n",
    "  \n",
    "clf_svc = GridSearchCV(svc, param_grid=params, refit = True, verbose = 1,scoring='f1_weighted') \n",
    "# fitting the model for grid search \n",
    "clf_svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Score: \", clf_svc.best_score_)\n",
    "print(\"Best Params: \", clf_svc.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_svc.best_estimator_.predict(X_test)\n",
    "#predictions = clf_svc.best_estimator_.predict_proba(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "#print (\"logloss: %0.3f \" % multiclass_logloss(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"LinearSVC_best_estimator_gcv\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### SGD Classifier\n",
    "\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"clf__loss\" : [\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\"],\n",
    "    \"clf__alpha\" : [0.0001, 0.001, 0.01, 0.1],\n",
    "    \"clf__penalty\" : [\"l2\", \"l1\", \"none\"],\n",
    "}\n",
    "\n",
    "clf_sgd = GridSearchCV(sgd, param_grid=params,refit = True, verbose = 1,scoring='f1_weighted')\n",
    "clf_sgd.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Score: \", clf_sgd.best_score_)\n",
    "print(\"Best Params: \", clf_sgd.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = clf_sgd.best_estimator_.predict(X_test)\n",
    "#predictions = clf_svc.best_estimator_.predict_proba(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "#print (\"logloss: %0.3f \" % multiclass_logloss(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"SGD_best_estimator_gcv\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {\n",
    "  'clf__penalty': ['l2'],\n",
    "  'clf__C': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0,1e2,1e4,1e5],\n",
    "  'clf__max_iter': [100,4000,5000],\n",
    "}\n",
    "\n",
    "clf_lr = GridSearchCV(logreg_1, param_grid=params,refit = True,verbose = 1,scoring='f1_weighted')\n",
    "clf_lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Score: \", clf_lr.best_score_)\n",
    "print(\"Best Params: \", clf_lr.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_lr.best_estimator_.predict(X_test)\n",
    "#predictions = clf_svc.best_estimator_.predict_proba(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred, y_test,average='weighted')) \n",
    "#print (\"logloss: %0.3f \" % multiclass_logloss(y_test,predictions))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"LogisticRegression_best_estimator_gcv\",accuracy_score(y_pred, y_test),f1_score(y_pred, y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Word2vec embedding and Logistic Regression\n",
    "# \n",
    "\n",
    "# Let's use pretrined glove embeddings to train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"dataset/glove.6B/glove.6B.100d.w2vformat.txt\")\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inc_sample.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_inc_sample, test_size=0.2, random_state = 13,stratify=df_inc_sample['Assignment_group'])\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['token_desc']), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['token_desc']), axis=1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Logistic Regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5,class_weight='balanced')\n",
    "logreg = logreg.fit(X_train_word_average, train['Assignment_group'])\n",
    "y_pred = logreg.predict(X_test_word_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,f1_score\n",
    "print('accuracy %s' % accuracy_score(y_pred, test.Assignment_group))\n",
    "print('f1 score %s' % f1_score(y_pred, test.Assignment_group,average='weighted'))\n",
    "print(classification_report(test.Assignment_group, y_pred))\n",
    "\n",
    "\n",
    "print(confusion_matrix(test.Assignment_group,y_pred))\n",
    "log_entry = pd.DataFrame([[\"Word2Vec - LogisticRegression\",accuracy_score(y_pred, test.Assignment_group),f1_score(y_pred, test.Assignment_group,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <b> We see that the performance is very poor than the benchmark model, as the pretrained embedding used is not specific to ITSM data and more related to generic english texts </b>\n",
    "\n",
    "# ### Xgboost\n",
    "\n",
    "\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "xgb_clf = xgb_clf.fit(X_train_word_average, train['Assignment_group'],sample_weight=w_array)\n",
    "y_pred = xgb_clf.predict(X_test_word_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,f1_score\n",
    "print('accuracy %s' % accuracy_score(y_pred, test.Assignment_group))\n",
    "print('f1 score %s' % f1_score(y_pred, test.Assignment_group,average='weighted'))\n",
    "print(classification_report(test.Assignment_group, y_pred))\n",
    "\n",
    "print(confusion_matrix(test.Assignment_group,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Doc2vec and Logistic Regression\n",
    "# \n",
    "# Let's try using Doc2vec, doc2vec is an extension to the word2vec-approach towards documents. Its intention is encode (whole) docs, consisting of lists of sentences, rather than lists of ungrouped sentences. \n",
    "# \n",
    "# There are two approaches in Doc2vec:\n",
    "#     - Distributed Memory\n",
    "#     - Distributed BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v.split(), tags=[label]))\n",
    "    return labeled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(df_inc_sample.token_desc, df_inc_sample.Assignment_group, test_size=0.2, random_state = 13,stratify=df_inc_sample['Assignment_group'])\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "# encoding train labels \n",
    "encoder.fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weights\n",
    "\n",
    "w_array = np.ones(y_train.shape[0], dtype = 'float')\n",
    "for i, val in enumerate(y_train):\n",
    "    w_array[i] = class_weights[val]\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "X_train[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Distributed BOW\n",
    "# DBOW is the doc2vec model analogous to Skip-gram model in word2vec. The paragraph vectors are obtained by training a neural network on the task of predicting a probability distribution of words in a paragraph given a randomly-sampled word from the paragraph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec(corpus):\n",
    "    logging.info('Building Doc2Vec model')\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "    model_dbow.build_vocab(corpus)\n",
    "    return model_dbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Building a Vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Training DBOW model for 30 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(40):\n",
    "    model_dbow.train(utils.shuffle([x for x in all_data]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Building the Final Vector Feature for the Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Train the Logistic Regression Classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5,class_weight='balanced')\n",
    "logreg.fit(train_vectors_dbow, y_train)\n",
    "y_pred = logreg.predict(test_vectors_dbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred,y_test,average='weighted'))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"Doc2Vec (dbow) - LogisticRegression\",accuracy_score(y_pred, y_test),f1_score(y_pred,y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "xgb_clf = xgb_clf.fit(train_vectors_dbow, y_train,sample_weight=w_array)\n",
    "y_pred = xgb_clf.predict(test_vectors_dbow)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "#print('f1 score %s' % f1_score(y_pred,y_test,average='weighted'))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed Memory (DM) acts as a memory that remembers what is missing from the current context — or as the topic of the paragraph. While the word vectors represent the concept of a word, the document vector intends to represent the concept of a document. We again instantiate a Doc2Vec model with a vector size with 300 words and iterating over the training corpus 30 times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_dm = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=3, negative=5, min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "model_dm.build_vocab([x for x in all_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(40):\n",
    "    model_dm.train(utils.shuffle([x for x in all_data]), total_examples=len(all_data), epochs=1)\n",
    "    model_dm.alpha -= 0.002\n",
    "    model_dm.min_alpha = model_dm.alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors_dm = get_vectors(model_dm, len(X_train), 300, 'Train')\n",
    "test_vectors_dm = get_vectors(model_dm, len(X_test), 300, 'Test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = logreg.fit(train_vectors_dm, y_train)\n",
    "y_pred = logreg.predict(test_vectors_dm)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred,y_test,average='weighted'))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"Doc2Vec (dm) - LogisticRegression\",accuracy_score(y_pred, y_test),f1_score(y_pred,y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "xgb_clf = xgb_clf.fit(train_vectors_dm, y_train,sample_weight=w_array)\n",
    "y_pred = xgb_clf.predict(test_vectors_dm)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print('f1 score %s' % f1_score(y_pred,y_test,average='weighted'))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also tried combining a paragraph vector from Distributed Bag of Words (DBOW) and Distributed Memory (DM) together for evaluation to check if it improves performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "model_dm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Concatenate two models with feature vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concat_vectors(model1,model2, corpus_size, vectors_size, vectors_type):\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = np.append(model1.docvecs[prefix],model2.docvecs[prefix])\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_vecs_dbow_dm = get_concat_vectors(model_dbow,model_dm, len(X_train), 600, 'Train')\n",
    "test_vecs_dbow_dm = get_concat_vectors(model_dbow,model_dm, len(X_test), 600, 'Test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Train Logistic Regression Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = logreg.fit(train_vecs_dbow_dm, y_train)\n",
    "y_pred = logreg.predict(test_vecs_dbow_dm)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "log_entry = pd.DataFrame([[\"Doc2Vec (dbow + dm) - LogisticRegression\",accuracy_score(y_pred, y_test),f1_score(y_pred,y_test,average='weighted')]], columns=log_cols)\n",
    "log = log.append(log_entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n",
    "                        subsample=0.8, nthread=10, learning_rate=0.1)\n",
    "xgb_clf = xgb_clf.fit(train_vecs_dbow_dm, y_train)\n",
    "y_pred = xgb_clf.predict(test_vecs_dbow_dm)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <b> We dont see much improvement in the performance using these word embeddings compared to the benchmark model using TFID approach <\\b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log.set_index([\"Classifier\"],inplace=True)\n",
    "log.sort_values(by=['f1_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.sort_values(by=['f1_score']).plot(kind='barh',figsize=[7,6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy import spatial\n",
    "sentence = ['The job did not start this morning on time']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder.inverse_transform(xgboost.predict(sentence)))\n",
    "print(encoder.inverse_transform(logreg_1.predict(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(logreg_1, '..dataset/auto_ticket_assignment.pkl', compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_name_mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "le_name_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "model = joblib.load('../dataset/auto_ticket_assignment.pkl')\n",
    "\n",
    "sentence = 'not able to connect to my system'\n",
    "encoder.inverse_transform(model.predict([sentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Finals Conclusions fo Approach1\n",
    "# \n",
    "# - We first analysed the dataset provided to us, undestood the structure of the data provided - number of columns, field , datatypes etc.\n",
    "# - We did Exploratory Data Analysis to derive further insights from this data set and we found that\n",
    "#     - Data is very much imbalanced, there are around ~45% of the Groups with less than 20 tickets.\n",
    "#     - Few of the tickets are in foreign language like German\n",
    "#     - The data has lot of noise in it, for eg- few tickets related to account setup are spread across multiple assignment groups.\n",
    "#     \n",
    "# - We performed the data cleaning and preprocessing\n",
    "#     - Translation: A small number of tickets were written in German. Hence, we used the Google translate python api  to convert German to English to generate the input data for the next steps. However, the google translator rest api can only process a limited number of texts on a daily basis, so we translated the text in batches and saved the file for further processing.\n",
    "#     - Make text all lowercase so that the algorithm does not treat the same words in different cases as different\n",
    "#     - Removing Noise i.e everything that isn’t in a standard number or letter i.e Punctuation, Numerical values\n",
    "#     - Removing extract spaces\n",
    "#     - Removed punctuations\n",
    "#     - Removed words containing numbers\n",
    "#     - Stopword Removal: Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words\n",
    "#     - Lemmatization\n",
    "#     - Tokenization: Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e words that we actually want. Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings.\n",
    "#     \n",
    "# \n",
    "# - We then ran a basic benchmarck model using the cleaned and preprocessed dataset\n",
    "#     - Since the dataset is very imbalanced, We considered a subset of groups for predictions.  In 74 groups, 46% of tickets belong to group 1 and 16 groups just have more than 100 tickets, rest of the Assignment groups have very less ticket counts which might not add much value to the model prediction. If we conducted random sampling towards all the subcategories, then we would face a problem that we might miss all the tickets in some categories. Hence, we considered the groups that have more than 100 tickets. \n",
    "#     - We trained the data using below models:\n",
    "#         - Multinomial NB\n",
    "#         - Linear Support vector Machine\n",
    "#         - Logistic regression\n",
    "#         - Xgboost\n",
    "#         \n",
    "# - LinearSVC gives better performance with \n",
    "#     - accuracy 0.833642\n",
    "#     - f1 score 0.818053\n",
    "# \n",
    "# <b> Although, it seems like the call is biased towards GRP_0 which has a majority of samples. </b>\n",
    "# \n",
    "# \n",
    "# \n",
    "# - Even after downsampling the data we see that the predictions are biased towards GRP_0 which has a majority of samples.\n",
    "# - Imbalance causes two problems:\n",
    "#     - Training is inefficient as most samples are easy examples that contribute no useful learning signal;\n",
    "#     - The easy examples can overwhelm training and lead to degenerate models.\n",
    "#     A common solution is to perform some form of hard negative mining that samples hard examples during training or more complex sampling/re weighing schemes.In order to handle the imbalance problem  we used class_weight=balanced hyperparameter while training the model, which tells the model to \"pay more attention\" to samples from an under-represented class.  \n",
    "# - Although, the accuracy and f1_score went down. This ensured that the classes were being correctly classified with lesser number of missclassification and good precision/recall scores for all the classes\n",
    "# \n",
    "# - Next, we also tried using pretrained word embedding, but the only challenge was that we could not find any embeddings trained on ITSM data. We used the glove model with 100d for this, and then used logistic regression and Xgboost to train the model. But, the scores were poorer than the benchmark model.\n",
    "#  \n",
    "# - Then, we also tried vector space modelling using Doc2Vec with DistributedBOW and Distributed Memory approach, though ‘Doc2Vec’ is a more advanced model in NLP rather than ‘Tf-Idf’, but still in our case, it is not giving proper results. We have tried with a linear  & boosting based classifier respectively.\n",
    "#   \n",
    "#   In our dataset, ‘texts’ are domain-specific. Furthermore, ‘Doc2Vec’ model is more suitable for very well written grammatically correct texts. In our case, texts are quite rough in nature.It is also proven in various examples and Data Sc ientist’s experiments that though ‘Tf-Idf’ model is inferior as compared to ‘Doc2Vec’, but still it gives better result while classifying very domain specific texts.\n",
    "#  \n",
    "#  \n",
    "#  - Linear SVC gave better performance with hyperparameter tuning and this model would be used for classifying the tickets into one of the groups.\n",
    "#     - accuracy 0.797441\n",
    "#     - f1 score 0.797100\n",
    "# \n",
    "# The performance can be further improved by collecting more data for tickets and by running deep learning models like RNN and LSTM's.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
